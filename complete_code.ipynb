{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MODELOS",
   "id": "98cad18a03fac0dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    content: str\n",
    "    metadata: Optional[dict] = None\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    id: str\n",
    "    document_id: str\n",
    "    content: str\n",
    "    embedding: Optional[List[float]] = None\n",
    "\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "    top_k: int = 3\n",
    "\n",
    "class RetrievedDocument(BaseModel):\n",
    "    chunk: Chunk\n",
    "    relevance_score: float\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    retrieved_documents: List[RetrievedDocument]\n",
    "    processing_time: Optional[float] = None"
   ],
   "id": "211eda69e36c8c15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CARREGAR DOCUMENTO",
   "id": "c5622e71814a15a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "from core.models import Document\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "\n",
    "class DocumentLoader:\n",
    "    @staticmethod\n",
    "    def load_docx2txt(file_path: str) -> str:\n",
    "        doc = docx2txt.Document(file_path)\n",
    "        return '\\n'.join([p.text for p in doc.paragraphs if p.text.strip()])\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pdf(file_path: str) -> str:\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def load_documents(paths: List[str]) -> List[Document]:\n",
    "        docs = []\n",
    "        for path in paths:\n",
    "            ext = Path(path).suffix.lower()\n",
    "            if ext == '.pdf':\n",
    "                content = DocumentLoader.load_pdf(path)\n",
    "            elif ext == '.docx2txt':\n",
    "                content = DocumentLoader.load_docx2txt(path)\n",
    "            else:\n",
    "                continue\n",
    "            docs.append(Document(id=str(path), content=content))\n",
    "        return docs\n"
   ],
   "id": "8d2b635cede671fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EMBEDDING",
   "id": "e868b8c0df1f7a1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=False)\n",
    "        # Garante que cada embedding está na CPU e como lista de floats\n",
    "        return [emb.cpu().numpy().tolist() if hasattr(emb, 'cpu') else emb for emb in embeddings]\n"
   ],
   "id": "bcf1232066fb98e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LLM",
   "id": "28a094c3a6164fbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class HuggingFaceLLM:\n",
    "    def __init__(self, model_name: str = 'google/flan-t5-base'):\n",
    "        self.generator = pipeline('text2text-generation', model=model_name)\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        # Limitar o prompt para não exceder o limite do modelo\n",
    "        max_input_length = 400  # Deixar margem de segurança\n",
    "        if len(prompt) > max_input_length:\n",
    "            prompt = prompt[:max_input_length] + \"...\"\n",
    "\n",
    "        result = self.generator(prompt, max_new_tokens=1024, max_length=4096)\n",
    "        return result[0]['generated_text'] if 'generated_text' in result[0] else result[0]['text']\n"
   ],
   "id": "43c0e5f829100ede"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# VETORIZANDO TEXTO",
   "id": "b4a80c277124e10f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from core.models import Chunk\n",
    "\n",
    "class FAISSVectorStore:\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.chunks = []\n",
    "\n",
    "    def add_chunks(self, chunks: List[Chunk]):\n",
    "        embeddings = np.array([c.embedding for c in chunks]).astype('float32')\n",
    "        self.index.add(embeddings)\n",
    "        self.chunks.extend(chunks)\n",
    "\n",
    "    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Tuple[Chunk, float]]:\n",
    "        query = np.array([query_embedding]).astype('float32')\n",
    "        D, I = self.index.search(query, top_k)\n",
    "        results = []\n",
    "        for idx, score in zip(I[0], D[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                results.append((self.chunks[idx], score))\n",
    "        return results"
   ],
   "id": "ad927feccaa2e5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PIPELINES DO RAG",
   "id": "d3f15d8b6a662672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from core.document_loader import DocumentLoader\n",
    "from core.embedding import EmbeddingModel\n",
    "from core.vector_store import FAISSVectorStore\n",
    "from core.llm import HuggingFaceLLM\n",
    "from core.models import Query, Chunk, RetrievedDocument, Answer, Document\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, embedding_model_name: str = None, llm_model_name: str = None, embedding_dim: int = 384):\n",
    "        self.embedding = EmbeddingModel(model_name=embedding_model_name or 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "        self.vector_store = FAISSVectorStore(embedding_dim)\n",
    "        self.llm = HuggingFaceLLM(model_name=llm_model_name or 'google/flan-t5-base')\n",
    "        self.documents = []\n",
    "\n",
    "    def add_documents(self, paths: List[str]):\n",
    "        docs = DocumentLoader.load_documents(paths)\n",
    "        chunks = []\n",
    "        for doc in docs:\n",
    "            # Dividir documento em chunks menores\n",
    "            doc_chunks = self._split_document(doc)\n",
    "            for i, chunk_content in enumerate(doc_chunks):\n",
    "                chunk = Chunk(id=f\"{doc.id}_chunk_{i}\", document_id=doc.id, content=chunk_content)\n",
    "                chunk.embedding = self.embedding.embed([chunk.content])[0]\n",
    "                chunks.append(chunk)\n",
    "        self.vector_store.add_chunks(chunks)\n",
    "        self.documents.extend(docs)\n",
    "        return docs\n",
    "\n",
    "    def _split_document(self, doc: Document, chunk_size: int = 1000) -> List[str]:\n",
    "        \"\"\"Divide um documento em chunks menores baseado no número de caracteres\"\"\"\n",
    "        content = doc.content\n",
    "        chunks = []\n",
    "\n",
    "        # Dividir por parágrafos primeiro (se possível)\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # Se o parágrafo sozinho já é muito grande, dividi-lo\n",
    "            if len(paragraph) > chunk_size:\n",
    "                # Salvar chunk atual se não estiver vazio\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "\n",
    "                # Dividir parágrafo grande em pedaços menores\n",
    "                words = paragraph.split()\n",
    "                temp_chunk = \"\"\n",
    "                for word in words:\n",
    "                    if len(temp_chunk + \" \" + word) > chunk_size and temp_chunk:\n",
    "                        chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word\n",
    "                    else:\n",
    "                        temp_chunk += \" \" + word if temp_chunk else word\n",
    "\n",
    "                if temp_chunk.strip():\n",
    "                    chunks.append(temp_chunk.strip())\n",
    "\n",
    "            # Se adicionar este parágrafo não exceder o limite\n",
    "            elif len(current_chunk + \"\\n\\n\" + paragraph) <= chunk_size:\n",
    "                current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "            else:\n",
    "                # Salvar chunk atual e começar novo\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "\n",
    "        # Adicionar último chunk se não estiver vazio\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks if chunks else [content[:chunk_size]]\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5) -> Answer:\n",
    "        start = time.time()\n",
    "        query_embedding = self.embedding.embed([question])[0]\n",
    "        results = self.vector_store.search(query_embedding, top_k=top_k)\n",
    "        retrieved = [RetrievedDocument(chunk=chunk, relevance_score=score) for chunk, score in results]\n",
    "\n",
    "        # Limitar o contexto para não exceder o limite do modelo\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        max_context_length = 400  # Deixar espaço para pergunta e resposta\n",
    "\n",
    "        for r in retrieved:\n",
    "            chunk_text = r.chunk.content\n",
    "            if total_length + len(chunk_text) <= max_context_length:\n",
    "                context_parts.append(chunk_text)\n",
    "                total_length += len(chunk_text)\n",
    "            else:\n",
    "                # Adicionar parte do chunk se couber\n",
    "                remaining = max_context_length - total_length\n",
    "                if remaining > 50:  # Só adicionar se for significativo\n",
    "                    context_parts.append(chunk_text[:remaining] + \"...\")\n",
    "                break\n",
    "\n",
    "        context = '\\n'.join(context_parts)\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        answer_text = self.llm.generate(prompt)\n",
    "        end = time.time()\n",
    "        return Answer(answer=answer_text, retrieved_documents=retrieved, processing_time=end-start)"
   ],
   "id": "2c423e38228f0420"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RODANDO APLICAÇÃO",
   "id": "ec5b494eeb015300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "from core.rag_pipeline import RAGPipeline\n",
    "from config.settings import settings\n",
    "from utils.helpers import setup_logging, format_duration\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    rag = RAGPipeline(\n",
    "        embedding_model_name=settings.embedding_model_name,\n",
    "        llm_model_name=settings.llm_model_name,\n",
    "        embedding_dim=settings.embedding_dim\n",
    "    )\n",
    "    # Exemplo: adicionar documentos\n",
    "    paths = [\"doencas_respiratorias_cronicas.pdf\"]\n",
    "    rag.add_documents(paths)\n",
    "    # Exemplo: consulta\n",
    "    question = \"Qual é a prevalência e o impacto das principais doenças respiratórias crônicas no Brasil, segundo dados do Ministério da Saúde?\"\n",
    "    answer = rag.query(question)\n",
    "    print(f\"Resposta: {answer.answer}\")\n",
    "    print(f\"Tempo de processamento: {format_duration(answer.processing_time)}\")\n",
    "    print(f\"Documentos recuperados: {len(answer.retrieved_documents)}\")"
   ],
   "id": "5f21297c61dca01f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
